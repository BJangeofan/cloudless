I0102 20:28:38.774893 1965929216 caffe.cpp:184] Using GPUs 0
I0102 20:28:39.874618 1965929216 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 50
base_lr: 0.001
display: 20
max_iter: 500
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 25000
snapshot: 100
snapshot_prefix: "snapshots/bvlc_alexnet"
solver_mode: GPU
device_id: 0
net: "src/caffe_model/bvlc_alexnet/train_val.prototxt"
I0102 20:28:39.875254 1965929216 solver.cpp:91] Creating training net from net file: src/caffe_model/bvlc_alexnet/train_val.prototxt
I0102 20:28:39.876725 1965929216 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0102 20:28:39.876751 1965929216 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0102 20:28:39.876761 1965929216 net.cpp:49] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "data/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "data/leveldb/train_leveldb"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_cloudless"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_cloudless"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_cloudless"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_cloudless"
  bottom: "label"
  top: "loss"
}
I0102 20:28:39.877466 1965929216 layer_factory.hpp:77] Creating layer data
I0102 20:28:39.882007 1965929216 net.cpp:106] Creating Layer data
I0102 20:28:39.882027 1965929216 net.cpp:411] data -> data
I0102 20:28:39.882048 1965929216 net.cpp:411] data -> label
I0102 20:28:39.882062 1965929216 data_transformer.cpp:25] Loading mean file from: data/imagenet/imagenet_mean.binaryproto
I0102 20:28:39.889354 317591552 db_leveldb.cpp:18] Opened leveldb data/leveldb/train_leveldb
I0102 20:28:39.912053 1965929216 data_layer.cpp:41] output data size: 64,3,227,227
I0102 20:28:40.064810 1965929216 net.cpp:150] Setting up data
I0102 20:28:40.064851 1965929216 net.cpp:157] Top shape: 64 3 227 227 (9893568)
I0102 20:28:40.064860 1965929216 net.cpp:157] Top shape: 64 (64)
I0102 20:28:40.064865 1965929216 net.cpp:165] Memory required for data: 39574528
I0102 20:28:40.064880 1965929216 layer_factory.hpp:77] Creating layer label_data_1_split
I0102 20:28:40.064898 1965929216 net.cpp:106] Creating Layer label_data_1_split
I0102 20:28:40.064905 1965929216 net.cpp:454] label_data_1_split <- label
I0102 20:28:40.064923 1965929216 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0102 20:28:40.064934 1965929216 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0102 20:28:40.064988 1965929216 net.cpp:150] Setting up label_data_1_split
I0102 20:28:40.064998 1965929216 net.cpp:157] Top shape: 64 (64)
I0102 20:28:40.065017 1965929216 net.cpp:157] Top shape: 64 (64)
I0102 20:28:40.065027 1965929216 net.cpp:165] Memory required for data: 39575040
I0102 20:28:40.065035 1965929216 layer_factory.hpp:77] Creating layer conv1
I0102 20:28:40.065053 1965929216 net.cpp:106] Creating Layer conv1
I0102 20:28:40.065060 1965929216 net.cpp:454] conv1 <- data
I0102 20:28:40.065068 1965929216 net.cpp:411] conv1 -> conv1
I0102 20:28:40.432097 1965929216 net.cpp:150] Setting up conv1
I0102 20:28:40.432119 1965929216 net.cpp:157] Top shape: 64 96 55 55 (18585600)
I0102 20:28:40.432126 1965929216 net.cpp:165] Memory required for data: 113917440
I0102 20:28:40.432137 1965929216 layer_factory.hpp:77] Creating layer relu1
I0102 20:28:40.432149 1965929216 net.cpp:106] Creating Layer relu1
I0102 20:28:40.432154 1965929216 net.cpp:454] relu1 <- conv1
I0102 20:28:40.432163 1965929216 net.cpp:397] relu1 -> conv1 (in-place)
I0102 20:28:40.432375 1965929216 net.cpp:150] Setting up relu1
I0102 20:28:40.432386 1965929216 net.cpp:157] Top shape: 64 96 55 55 (18585600)
I0102 20:28:40.432391 1965929216 net.cpp:165] Memory required for data: 188259840
I0102 20:28:40.432396 1965929216 layer_factory.hpp:77] Creating layer norm1
I0102 20:28:40.432410 1965929216 net.cpp:106] Creating Layer norm1
I0102 20:28:40.432416 1965929216 net.cpp:454] norm1 <- conv1
I0102 20:28:40.432422 1965929216 net.cpp:411] norm1 -> norm1
I0102 20:28:40.433189 1965929216 net.cpp:150] Setting up norm1
I0102 20:28:40.433200 1965929216 net.cpp:157] Top shape: 64 96 55 55 (18585600)
I0102 20:28:40.433207 1965929216 net.cpp:165] Memory required for data: 262602240
I0102 20:28:40.433210 1965929216 layer_factory.hpp:77] Creating layer pool1
I0102 20:28:40.433218 1965929216 net.cpp:106] Creating Layer pool1
I0102 20:28:40.433223 1965929216 net.cpp:454] pool1 <- norm1
I0102 20:28:40.433228 1965929216 net.cpp:411] pool1 -> pool1
I0102 20:28:40.435333 1965929216 net.cpp:150] Setting up pool1
I0102 20:28:40.435339 1965929216 net.cpp:157] Top shape: 64 96 27 27 (4478976)
I0102 20:28:40.435343 1965929216 net.cpp:165] Memory required for data: 280518144
I0102 20:28:40.435348 1965929216 layer_factory.hpp:77] Creating layer conv2
I0102 20:28:40.435356 1965929216 net.cpp:106] Creating Layer conv2
I0102 20:28:40.435360 1965929216 net.cpp:454] conv2 <- pool1
I0102 20:28:40.435369 1965929216 net.cpp:411] conv2 -> conv2
I0102 20:28:40.440968 1965929216 net.cpp:150] Setting up conv2
I0102 20:28:40.440982 1965929216 net.cpp:157] Top shape: 64 256 27 27 (11943936)
I0102 20:28:40.440989 1965929216 net.cpp:165] Memory required for data: 328293888
I0102 20:28:40.440999 1965929216 layer_factory.hpp:77] Creating layer relu2
I0102 20:28:40.441015 1965929216 net.cpp:106] Creating Layer relu2
I0102 20:28:40.441018 1965929216 net.cpp:454] relu2 <- conv2
I0102 20:28:40.441025 1965929216 net.cpp:397] relu2 -> conv2 (in-place)
I0102 20:28:40.441169 1965929216 net.cpp:150] Setting up relu2
I0102 20:28:40.441176 1965929216 net.cpp:157] Top shape: 64 256 27 27 (11943936)
I0102 20:28:40.441184 1965929216 net.cpp:165] Memory required for data: 376069632
I0102 20:28:40.441191 1965929216 layer_factory.hpp:77] Creating layer norm2
I0102 20:28:40.441200 1965929216 net.cpp:106] Creating Layer norm2
I0102 20:28:40.441203 1965929216 net.cpp:454] norm2 <- conv2
I0102 20:28:40.441211 1965929216 net.cpp:411] norm2 -> norm2
I0102 20:28:40.441417 1965929216 net.cpp:150] Setting up norm2
I0102 20:28:40.441426 1965929216 net.cpp:157] Top shape: 64 256 27 27 (11943936)
I0102 20:28:40.441431 1965929216 net.cpp:165] Memory required for data: 423845376
I0102 20:28:40.441436 1965929216 layer_factory.hpp:77] Creating layer pool2
I0102 20:28:40.441440 1965929216 net.cpp:106] Creating Layer pool2
I0102 20:28:40.441445 1965929216 net.cpp:454] pool2 <- norm2
I0102 20:28:40.441452 1965929216 net.cpp:411] pool2 -> pool2
I0102 20:28:40.441596 1965929216 net.cpp:150] Setting up pool2
I0102 20:28:40.441601 1965929216 net.cpp:157] Top shape: 64 256 13 13 (2768896)
I0102 20:28:40.441606 1965929216 net.cpp:165] Memory required for data: 434920960
I0102 20:28:40.441609 1965929216 layer_factory.hpp:77] Creating layer conv3
I0102 20:28:40.441618 1965929216 net.cpp:106] Creating Layer conv3
I0102 20:28:40.441623 1965929216 net.cpp:454] conv3 <- pool2
I0102 20:28:40.441632 1965929216 net.cpp:411] conv3 -> conv3
I0102 20:28:40.454634 1965929216 net.cpp:150] Setting up conv3
I0102 20:28:40.454654 1965929216 net.cpp:157] Top shape: 64 384 13 13 (4153344)
I0102 20:28:40.454660 1965929216 net.cpp:165] Memory required for data: 451534336
I0102 20:28:40.454670 1965929216 layer_factory.hpp:77] Creating layer relu3
I0102 20:28:40.454681 1965929216 net.cpp:106] Creating Layer relu3
I0102 20:28:40.454686 1965929216 net.cpp:454] relu3 <- conv3
I0102 20:28:40.454692 1965929216 net.cpp:397] relu3 -> conv3 (in-place)
I0102 20:28:40.454866 1965929216 net.cpp:150] Setting up relu3
I0102 20:28:40.454874 1965929216 net.cpp:157] Top shape: 64 384 13 13 (4153344)
I0102 20:28:40.454905 1965929216 net.cpp:165] Memory required for data: 468147712
I0102 20:28:40.454910 1965929216 layer_factory.hpp:77] Creating layer conv4
I0102 20:28:40.454921 1965929216 net.cpp:106] Creating Layer conv4
I0102 20:28:40.454926 1965929216 net.cpp:454] conv4 <- conv3
I0102 20:28:40.454932 1965929216 net.cpp:411] conv4 -> conv4
I0102 20:28:40.464989 1965929216 net.cpp:150] Setting up conv4
I0102 20:28:40.465015 1965929216 net.cpp:157] Top shape: 64 384 13 13 (4153344)
I0102 20:28:40.465024 1965929216 net.cpp:165] Memory required for data: 484761088
I0102 20:28:40.465034 1965929216 layer_factory.hpp:77] Creating layer relu4
I0102 20:28:40.465044 1965929216 net.cpp:106] Creating Layer relu4
I0102 20:28:40.465092 1965929216 net.cpp:454] relu4 <- conv4
I0102 20:28:40.465104 1965929216 net.cpp:397] relu4 -> conv4 (in-place)
I0102 20:28:40.465323 1965929216 net.cpp:150] Setting up relu4
I0102 20:28:40.465332 1965929216 net.cpp:157] Top shape: 64 384 13 13 (4153344)
I0102 20:28:40.465337 1965929216 net.cpp:165] Memory required for data: 501374464
I0102 20:28:40.465342 1965929216 layer_factory.hpp:77] Creating layer conv5
I0102 20:28:40.465353 1965929216 net.cpp:106] Creating Layer conv5
I0102 20:28:40.465457 1965929216 net.cpp:454] conv5 <- conv4
I0102 20:28:40.465477 1965929216 net.cpp:411] conv5 -> conv5
I0102 20:28:40.473772 1965929216 net.cpp:150] Setting up conv5
I0102 20:28:40.473793 1965929216 net.cpp:157] Top shape: 64 256 13 13 (2768896)
I0102 20:28:40.473803 1965929216 net.cpp:165] Memory required for data: 512450048
I0102 20:28:40.473817 1965929216 layer_factory.hpp:77] Creating layer relu5
I0102 20:28:40.473831 1965929216 net.cpp:106] Creating Layer relu5
I0102 20:28:40.473839 1965929216 net.cpp:454] relu5 <- conv5
I0102 20:28:40.473855 1965929216 net.cpp:397] relu5 -> conv5 (in-place)
I0102 20:28:40.474113 1965929216 net.cpp:150] Setting up relu5
I0102 20:28:40.474130 1965929216 net.cpp:157] Top shape: 64 256 13 13 (2768896)
I0102 20:28:40.474146 1965929216 net.cpp:165] Memory required for data: 523525632
I0102 20:28:40.474153 1965929216 layer_factory.hpp:77] Creating layer pool5
I0102 20:28:40.474166 1965929216 net.cpp:106] Creating Layer pool5
I0102 20:28:40.474174 1965929216 net.cpp:454] pool5 <- conv5
I0102 20:28:40.474190 1965929216 net.cpp:411] pool5 -> pool5
I0102 20:28:40.474416 1965929216 net.cpp:150] Setting up pool5
I0102 20:28:40.474426 1965929216 net.cpp:157] Top shape: 64 256 6 6 (589824)
I0102 20:28:40.474434 1965929216 net.cpp:165] Memory required for data: 525884928
I0102 20:28:40.474442 1965929216 layer_factory.hpp:77] Creating layer fc6
I0102 20:28:40.474452 1965929216 net.cpp:106] Creating Layer fc6
I0102 20:28:40.474458 1965929216 net.cpp:454] fc6 <- pool5
I0102 20:28:40.474467 1965929216 net.cpp:411] fc6 -> fc6
I0102 20:28:41.017444 1965929216 net.cpp:150] Setting up fc6
I0102 20:28:41.017467 1965929216 net.cpp:157] Top shape: 64 4096 (262144)
I0102 20:28:41.017483 1965929216 net.cpp:165] Memory required for data: 526933504
I0102 20:28:41.017586 1965929216 layer_factory.hpp:77] Creating layer relu6
I0102 20:28:41.017601 1965929216 net.cpp:106] Creating Layer relu6
I0102 20:28:41.017606 1965929216 net.cpp:454] relu6 <- fc6
I0102 20:28:41.017613 1965929216 net.cpp:397] relu6 -> fc6 (in-place)
I0102 20:28:41.017943 1965929216 net.cpp:150] Setting up relu6
I0102 20:28:41.017951 1965929216 net.cpp:157] Top shape: 64 4096 (262144)
I0102 20:28:41.017956 1965929216 net.cpp:165] Memory required for data: 527982080
I0102 20:28:41.017961 1965929216 layer_factory.hpp:77] Creating layer drop6
I0102 20:28:41.017967 1965929216 net.cpp:106] Creating Layer drop6
I0102 20:28:41.017971 1965929216 net.cpp:454] drop6 <- fc6
I0102 20:28:41.017976 1965929216 net.cpp:397] drop6 -> fc6 (in-place)
I0102 20:28:41.018007 1965929216 net.cpp:150] Setting up drop6
I0102 20:28:41.018018 1965929216 net.cpp:157] Top shape: 64 4096 (262144)
I0102 20:28:41.018023 1965929216 net.cpp:165] Memory required for data: 529030656
I0102 20:28:41.018028 1965929216 layer_factory.hpp:77] Creating layer fc7
I0102 20:28:41.018065 1965929216 net.cpp:106] Creating Layer fc7
I0102 20:28:41.018070 1965929216 net.cpp:454] fc7 <- fc6
I0102 20:28:41.018076 1965929216 net.cpp:411] fc7 -> fc7
I0102 20:28:41.263643 1965929216 net.cpp:150] Setting up fc7
I0102 20:28:41.263664 1965929216 net.cpp:157] Top shape: 64 4096 (262144)
I0102 20:28:41.263670 1965929216 net.cpp:165] Memory required for data: 530079232
I0102 20:28:41.263677 1965929216 layer_factory.hpp:77] Creating layer relu7
I0102 20:28:41.263687 1965929216 net.cpp:106] Creating Layer relu7
I0102 20:28:41.263692 1965929216 net.cpp:454] relu7 <- fc7
I0102 20:28:41.263698 1965929216 net.cpp:397] relu7 -> fc7 (in-place)
I0102 20:28:41.263988 1965929216 net.cpp:150] Setting up relu7
I0102 20:28:41.263998 1965929216 net.cpp:157] Top shape: 64 4096 (262144)
I0102 20:28:41.264011 1965929216 net.cpp:165] Memory required for data: 531127808
I0102 20:28:41.264015 1965929216 layer_factory.hpp:77] Creating layer drop7
I0102 20:28:41.264022 1965929216 net.cpp:106] Creating Layer drop7
I0102 20:28:41.264026 1965929216 net.cpp:454] drop7 <- fc7
I0102 20:28:41.264031 1965929216 net.cpp:397] drop7 -> fc7 (in-place)
I0102 20:28:41.264055 1965929216 net.cpp:150] Setting up drop7
I0102 20:28:41.264060 1965929216 net.cpp:157] Top shape: 64 4096 (262144)
I0102 20:28:41.264063 1965929216 net.cpp:165] Memory required for data: 532176384
I0102 20:28:41.264067 1965929216 layer_factory.hpp:77] Creating layer fc8_cloudless
I0102 20:28:41.264076 1965929216 net.cpp:106] Creating Layer fc8_cloudless
I0102 20:28:41.264081 1965929216 net.cpp:454] fc8_cloudless <- fc7
I0102 20:28:41.264086 1965929216 net.cpp:411] fc8_cloudless -> fc8_cloudless
I0102 20:28:41.264479 1965929216 net.cpp:150] Setting up fc8_cloudless
I0102 20:28:41.264488 1965929216 net.cpp:157] Top shape: 64 2 (128)
I0102 20:28:41.264492 1965929216 net.cpp:165] Memory required for data: 532176896
I0102 20:28:41.264504 1965929216 layer_factory.hpp:77] Creating layer fc8_cloudless_fc8_cloudless_0_split
I0102 20:28:41.264513 1965929216 net.cpp:106] Creating Layer fc8_cloudless_fc8_cloudless_0_split
I0102 20:28:41.264518 1965929216 net.cpp:454] fc8_cloudless_fc8_cloudless_0_split <- fc8_cloudless
I0102 20:28:41.264524 1965929216 net.cpp:411] fc8_cloudless_fc8_cloudless_0_split -> fc8_cloudless_fc8_cloudless_0_split_0
I0102 20:28:41.264531 1965929216 net.cpp:411] fc8_cloudless_fc8_cloudless_0_split -> fc8_cloudless_fc8_cloudless_0_split_1
I0102 20:28:41.264564 1965929216 net.cpp:150] Setting up fc8_cloudless_fc8_cloudless_0_split
I0102 20:28:41.264569 1965929216 net.cpp:157] Top shape: 64 2 (128)
I0102 20:28:41.264574 1965929216 net.cpp:157] Top shape: 64 2 (128)
I0102 20:28:41.264577 1965929216 net.cpp:165] Memory required for data: 532177920
I0102 20:28:41.264581 1965929216 layer_factory.hpp:77] Creating layer accuracy
I0102 20:28:41.264591 1965929216 net.cpp:106] Creating Layer accuracy
I0102 20:28:41.264596 1965929216 net.cpp:454] accuracy <- fc8_cloudless_fc8_cloudless_0_split_0
I0102 20:28:41.264601 1965929216 net.cpp:454] accuracy <- label_data_1_split_0
I0102 20:28:41.264606 1965929216 net.cpp:411] accuracy -> accuracy
I0102 20:28:41.264616 1965929216 net.cpp:150] Setting up accuracy
I0102 20:28:41.264621 1965929216 net.cpp:157] Top shape: (1)
I0102 20:28:41.264624 1965929216 net.cpp:165] Memory required for data: 532177924
I0102 20:28:41.264628 1965929216 layer_factory.hpp:77] Creating layer loss
I0102 20:28:41.264638 1965929216 net.cpp:106] Creating Layer loss
I0102 20:28:41.264642 1965929216 net.cpp:454] loss <- fc8_cloudless_fc8_cloudless_0_split_1
I0102 20:28:41.264647 1965929216 net.cpp:454] loss <- label_data_1_split_1
I0102 20:28:41.264652 1965929216 net.cpp:411] loss -> loss
I0102 20:28:41.264658 1965929216 layer_factory.hpp:77] Creating layer loss
I0102 20:28:41.265027 1965929216 net.cpp:150] Setting up loss
I0102 20:28:41.265034 1965929216 net.cpp:157] Top shape: (1)
I0102 20:28:41.265038 1965929216 net.cpp:160]     with loss weight 1
I0102 20:28:41.265050 1965929216 net.cpp:165] Memory required for data: 532177928
I0102 20:28:41.265075 1965929216 net.cpp:226] loss needs backward computation.
I0102 20:28:41.265080 1965929216 net.cpp:228] accuracy does not need backward computation.
I0102 20:28:41.265084 1965929216 net.cpp:226] fc8_cloudless_fc8_cloudless_0_split needs backward computation.
I0102 20:28:41.265118 1965929216 net.cpp:226] fc8_cloudless needs backward computation.
I0102 20:28:41.265133 1965929216 net.cpp:226] drop7 needs backward computation.
I0102 20:28:41.265138 1965929216 net.cpp:226] relu7 needs backward computation.
I0102 20:28:41.265141 1965929216 net.cpp:226] fc7 needs backward computation.
I0102 20:28:41.265146 1965929216 net.cpp:228] drop6 does not need backward computation.
I0102 20:28:41.265149 1965929216 net.cpp:228] relu6 does not need backward computation.
I0102 20:28:41.265153 1965929216 net.cpp:228] fc6 does not need backward computation.
I0102 20:28:41.265185 1965929216 net.cpp:228] pool5 does not need backward computation.
I0102 20:28:41.265192 1965929216 net.cpp:228] relu5 does not need backward computation.
I0102 20:28:41.265199 1965929216 net.cpp:228] conv5 does not need backward computation.
I0102 20:28:41.265205 1965929216 net.cpp:228] relu4 does not need backward computation.
I0102 20:28:41.265209 1965929216 net.cpp:228] conv4 does not need backward computation.
I0102 20:28:41.265213 1965929216 net.cpp:228] relu3 does not need backward computation.
I0102 20:28:41.265220 1965929216 net.cpp:228] conv3 does not need backward computation.
I0102 20:28:41.265225 1965929216 net.cpp:228] pool2 does not need backward computation.
I0102 20:28:41.265230 1965929216 net.cpp:228] norm2 does not need backward computation.
I0102 20:28:41.265236 1965929216 net.cpp:228] relu2 does not need backward computation.
I0102 20:28:41.265241 1965929216 net.cpp:228] conv2 does not need backward computation.
I0102 20:28:41.265244 1965929216 net.cpp:228] pool1 does not need backward computation.
I0102 20:28:41.265249 1965929216 net.cpp:228] norm1 does not need backward computation.
I0102 20:28:41.265254 1965929216 net.cpp:228] relu1 does not need backward computation.
I0102 20:28:41.265257 1965929216 net.cpp:228] conv1 does not need backward computation.
I0102 20:28:41.265262 1965929216 net.cpp:228] label_data_1_split does not need backward computation.
I0102 20:28:41.265266 1965929216 net.cpp:228] data does not need backward computation.
I0102 20:28:41.265270 1965929216 net.cpp:270] This network produces output accuracy
I0102 20:28:41.265275 1965929216 net.cpp:270] This network produces output loss
I0102 20:28:41.265295 1965929216 net.cpp:283] Network initialization done.
I0102 20:28:41.265697 1965929216 solver.cpp:181] Creating test net (#0) specified by net file: src/caffe_model/bvlc_alexnet/train_val.prototxt
I0102 20:28:41.265774 1965929216 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0102 20:28:41.265792 1965929216 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy
I0102 20:28:41.265797 1965929216 net.cpp:49] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "data/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "data/leveldb/validation_leveldb"
    batch_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_cloudless"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_cloudless"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_cloudless"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_cloudless"
  bottom: "label"
  top: "loss"
}
I0102 20:28:41.266116 1965929216 layer_factory.hpp:77] Creating layer data
I0102 20:28:41.266260 1965929216 net.cpp:106] Creating Layer data
I0102 20:28:41.266276 1965929216 net.cpp:411] data -> data
I0102 20:28:41.266293 1965929216 net.cpp:411] data -> label
I0102 20:28:41.266301 1965929216 data_transformer.cpp:25] Loading mean file from: data/imagenet/imagenet_mean.binaryproto
I0102 20:28:41.274076 970072064 db_leveldb.cpp:18] Opened leveldb data/leveldb/validation_leveldb
I0102 20:28:41.274700 1965929216 data_layer.cpp:41] output data size: 50,3,227,227
I0102 20:28:41.442607 1965929216 net.cpp:150] Setting up data
I0102 20:28:41.442633 1965929216 net.cpp:157] Top shape: 50 3 227 227 (7729350)
I0102 20:28:41.442642 1965929216 net.cpp:157] Top shape: 50 (50)
I0102 20:28:41.442734 1965929216 net.cpp:165] Memory required for data: 30917600
I0102 20:28:41.442749 1965929216 layer_factory.hpp:77] Creating layer label_data_1_split
I0102 20:28:41.442762 1965929216 net.cpp:106] Creating Layer label_data_1_split
I0102 20:28:41.442770 1965929216 net.cpp:454] label_data_1_split <- label
I0102 20:28:41.442797 1965929216 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0102 20:28:41.442813 1965929216 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0102 20:28:41.442889 1965929216 net.cpp:150] Setting up label_data_1_split
I0102 20:28:41.442896 1965929216 net.cpp:157] Top shape: 50 (50)
I0102 20:28:41.442903 1965929216 net.cpp:157] Top shape: 50 (50)
I0102 20:28:41.442908 1965929216 net.cpp:165] Memory required for data: 30918000
I0102 20:28:41.442929 1965929216 layer_factory.hpp:77] Creating layer conv1
I0102 20:28:41.442950 1965929216 net.cpp:106] Creating Layer conv1
I0102 20:28:41.442955 1965929216 net.cpp:454] conv1 <- data
I0102 20:28:41.442963 1965929216 net.cpp:411] conv1 -> conv1
I0102 20:28:41.444450 1965929216 net.cpp:150] Setting up conv1
I0102 20:28:41.444461 1965929216 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I0102 20:28:41.444468 1965929216 net.cpp:165] Memory required for data: 88998000
I0102 20:28:41.444476 1965929216 layer_factory.hpp:77] Creating layer relu1
I0102 20:28:41.444484 1965929216 net.cpp:106] Creating Layer relu1
I0102 20:28:41.444489 1965929216 net.cpp:454] relu1 <- conv1
I0102 20:28:41.444495 1965929216 net.cpp:397] relu1 -> conv1 (in-place)
I0102 20:28:41.444638 1965929216 net.cpp:150] Setting up relu1
I0102 20:28:41.444644 1965929216 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I0102 20:28:41.444649 1965929216 net.cpp:165] Memory required for data: 147078000
I0102 20:28:41.444654 1965929216 layer_factory.hpp:77] Creating layer norm1
I0102 20:28:41.444663 1965929216 net.cpp:106] Creating Layer norm1
I0102 20:28:41.444666 1965929216 net.cpp:454] norm1 <- conv1
I0102 20:28:41.444703 1965929216 net.cpp:411] norm1 -> norm1
I0102 20:28:41.444980 1965929216 net.cpp:150] Setting up norm1
I0102 20:28:41.444990 1965929216 net.cpp:157] Top shape: 50 96 55 55 (14520000)
I0102 20:28:41.444995 1965929216 net.cpp:165] Memory required for data: 205158000
I0102 20:28:41.445000 1965929216 layer_factory.hpp:77] Creating layer pool1
I0102 20:28:41.445009 1965929216 net.cpp:106] Creating Layer pool1
I0102 20:28:41.445013 1965929216 net.cpp:454] pool1 <- norm1
I0102 20:28:41.445019 1965929216 net.cpp:411] pool1 -> pool1
I0102 20:28:41.445183 1965929216 net.cpp:150] Setting up pool1
I0102 20:28:41.445190 1965929216 net.cpp:157] Top shape: 50 96 27 27 (3499200)
I0102 20:28:41.445195 1965929216 net.cpp:165] Memory required for data: 219154800
I0102 20:28:41.445199 1965929216 layer_factory.hpp:77] Creating layer conv2
I0102 20:28:41.445209 1965929216 net.cpp:106] Creating Layer conv2
I0102 20:28:41.445214 1965929216 net.cpp:454] conv2 <- pool1
I0102 20:28:41.445220 1965929216 net.cpp:411] conv2 -> conv2
I0102 20:28:41.460054 1965929216 net.cpp:150] Setting up conv2
I0102 20:28:41.460072 1965929216 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I0102 20:28:41.460089 1965929216 net.cpp:165] Memory required for data: 256479600
I0102 20:28:41.460108 1965929216 layer_factory.hpp:77] Creating layer relu2
I0102 20:28:41.460117 1965929216 net.cpp:106] Creating Layer relu2
I0102 20:28:41.460145 1965929216 net.cpp:454] relu2 <- conv2
I0102 20:28:41.460152 1965929216 net.cpp:397] relu2 -> conv2 (in-place)
I0102 20:28:41.460352 1965929216 net.cpp:150] Setting up relu2
I0102 20:28:41.460361 1965929216 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I0102 20:28:41.460366 1965929216 net.cpp:165] Memory required for data: 293804400
I0102 20:28:41.460371 1965929216 layer_factory.hpp:77] Creating layer norm2
I0102 20:28:41.460378 1965929216 net.cpp:106] Creating Layer norm2
I0102 20:28:41.460382 1965929216 net.cpp:454] norm2 <- conv2
I0102 20:28:41.460388 1965929216 net.cpp:411] norm2 -> norm2
I0102 20:28:41.460544 1965929216 net.cpp:150] Setting up norm2
I0102 20:28:41.460551 1965929216 net.cpp:157] Top shape: 50 256 27 27 (9331200)
I0102 20:28:41.460556 1965929216 net.cpp:165] Memory required for data: 331129200
I0102 20:28:41.460561 1965929216 layer_factory.hpp:77] Creating layer pool2
I0102 20:28:41.460566 1965929216 net.cpp:106] Creating Layer pool2
I0102 20:28:41.460571 1965929216 net.cpp:454] pool2 <- norm2
I0102 20:28:41.460575 1965929216 net.cpp:411] pool2 -> pool2
I0102 20:28:41.460789 1965929216 net.cpp:150] Setting up pool2
I0102 20:28:41.460798 1965929216 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I0102 20:28:41.460803 1965929216 net.cpp:165] Memory required for data: 339782000
I0102 20:28:41.460806 1965929216 layer_factory.hpp:77] Creating layer conv3
I0102 20:28:41.460815 1965929216 net.cpp:106] Creating Layer conv3
I0102 20:28:41.460820 1965929216 net.cpp:454] conv3 <- pool2
I0102 20:28:41.460827 1965929216 net.cpp:411] conv3 -> conv3
I0102 20:28:41.474823 1965929216 net.cpp:150] Setting up conv3
I0102 20:28:41.474845 1965929216 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I0102 20:28:41.474853 1965929216 net.cpp:165] Memory required for data: 352761200
I0102 20:28:41.474863 1965929216 layer_factory.hpp:77] Creating layer relu3
I0102 20:28:41.474871 1965929216 net.cpp:106] Creating Layer relu3
I0102 20:28:41.474877 1965929216 net.cpp:454] relu3 <- conv3
I0102 20:28:41.474884 1965929216 net.cpp:397] relu3 -> conv3 (in-place)
I0102 20:28:41.475024 1965929216 net.cpp:150] Setting up relu3
I0102 20:28:41.475029 1965929216 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I0102 20:28:41.475035 1965929216 net.cpp:165] Memory required for data: 365740400
I0102 20:28:41.475039 1965929216 layer_factory.hpp:77] Creating layer conv4
I0102 20:28:41.475051 1965929216 net.cpp:106] Creating Layer conv4
I0102 20:28:41.475055 1965929216 net.cpp:454] conv4 <- conv3
I0102 20:28:41.475061 1965929216 net.cpp:411] conv4 -> conv4
I0102 20:28:41.485909 1965929216 net.cpp:150] Setting up conv4
I0102 20:28:41.485925 1965929216 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I0102 20:28:41.485931 1965929216 net.cpp:165] Memory required for data: 378719600
I0102 20:28:41.485939 1965929216 layer_factory.hpp:77] Creating layer relu4
I0102 20:28:41.485946 1965929216 net.cpp:106] Creating Layer relu4
I0102 20:28:41.485951 1965929216 net.cpp:454] relu4 <- conv4
I0102 20:28:41.485957 1965929216 net.cpp:397] relu4 -> conv4 (in-place)
I0102 20:28:41.486091 1965929216 net.cpp:150] Setting up relu4
I0102 20:28:41.486099 1965929216 net.cpp:157] Top shape: 50 384 13 13 (3244800)
I0102 20:28:41.486104 1965929216 net.cpp:165] Memory required for data: 391698800
I0102 20:28:41.486107 1965929216 layer_factory.hpp:77] Creating layer conv5
I0102 20:28:41.486116 1965929216 net.cpp:106] Creating Layer conv5
I0102 20:28:41.486120 1965929216 net.cpp:454] conv5 <- conv4
I0102 20:28:41.486126 1965929216 net.cpp:411] conv5 -> conv5
I0102 20:28:41.493510 1965929216 net.cpp:150] Setting up conv5
I0102 20:28:41.493528 1965929216 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I0102 20:28:41.493538 1965929216 net.cpp:165] Memory required for data: 400351600
I0102 20:28:41.493557 1965929216 layer_factory.hpp:77] Creating layer relu5
I0102 20:28:41.493571 1965929216 net.cpp:106] Creating Layer relu5
I0102 20:28:41.493578 1965929216 net.cpp:454] relu5 <- conv5
I0102 20:28:41.493587 1965929216 net.cpp:397] relu5 -> conv5 (in-place)
I0102 20:28:41.493825 1965929216 net.cpp:150] Setting up relu5
I0102 20:28:41.493844 1965929216 net.cpp:157] Top shape: 50 256 13 13 (2163200)
I0102 20:28:41.493856 1965929216 net.cpp:165] Memory required for data: 409004400
I0102 20:28:41.493865 1965929216 layer_factory.hpp:77] Creating layer pool5
I0102 20:28:41.493882 1965929216 net.cpp:106] Creating Layer pool5
I0102 20:28:41.493891 1965929216 net.cpp:454] pool5 <- conv5
I0102 20:28:41.493906 1965929216 net.cpp:411] pool5 -> pool5
I0102 20:28:41.494261 1965929216 net.cpp:150] Setting up pool5
I0102 20:28:41.494276 1965929216 net.cpp:157] Top shape: 50 256 6 6 (460800)
I0102 20:28:41.494292 1965929216 net.cpp:165] Memory required for data: 410847600
I0102 20:28:41.494300 1965929216 layer_factory.hpp:77] Creating layer fc6
I0102 20:28:41.494313 1965929216 net.cpp:106] Creating Layer fc6
I0102 20:28:41.494326 1965929216 net.cpp:454] fc6 <- pool5
I0102 20:28:41.494338 1965929216 net.cpp:411] fc6 -> fc6
I0102 20:28:42.063900 1965929216 net.cpp:150] Setting up fc6
I0102 20:28:42.063925 1965929216 net.cpp:157] Top shape: 50 4096 (204800)
I0102 20:28:42.063942 1965929216 net.cpp:165] Memory required for data: 411666800
I0102 20:28:42.063951 1965929216 layer_factory.hpp:77] Creating layer relu6
I0102 20:28:42.063962 1965929216 net.cpp:106] Creating Layer relu6
I0102 20:28:42.063968 1965929216 net.cpp:454] relu6 <- fc6
I0102 20:28:42.063974 1965929216 net.cpp:397] relu6 -> fc6 (in-place)
I0102 20:28:42.064182 1965929216 net.cpp:150] Setting up relu6
I0102 20:28:42.064204 1965929216 net.cpp:157] Top shape: 50 4096 (204800)
I0102 20:28:42.064218 1965929216 net.cpp:165] Memory required for data: 412486000
I0102 20:28:42.064224 1965929216 layer_factory.hpp:77] Creating layer drop6
I0102 20:28:42.064232 1965929216 net.cpp:106] Creating Layer drop6
I0102 20:28:42.064237 1965929216 net.cpp:454] drop6 <- fc6
I0102 20:28:42.064244 1965929216 net.cpp:397] drop6 -> fc6 (in-place)
I0102 20:28:42.064292 1965929216 net.cpp:150] Setting up drop6
I0102 20:28:42.064297 1965929216 net.cpp:157] Top shape: 50 4096 (204800)
I0102 20:28:42.064301 1965929216 net.cpp:165] Memory required for data: 413305200
I0102 20:28:42.064306 1965929216 layer_factory.hpp:77] Creating layer fc7
I0102 20:28:42.064313 1965929216 net.cpp:106] Creating Layer fc7
I0102 20:28:42.064317 1965929216 net.cpp:454] fc7 <- fc6
I0102 20:28:42.064337 1965929216 net.cpp:411] fc7 -> fc7
I0102 20:28:42.315789 1965929216 net.cpp:150] Setting up fc7
I0102 20:28:42.315824 1965929216 net.cpp:157] Top shape: 50 4096 (204800)
I0102 20:28:42.315836 1965929216 net.cpp:165] Memory required for data: 414124400
I0102 20:28:42.315850 1965929216 layer_factory.hpp:77] Creating layer relu7
I0102 20:28:42.315866 1965929216 net.cpp:106] Creating Layer relu7
I0102 20:28:42.315876 1965929216 net.cpp:454] relu7 <- fc7
I0102 20:28:42.315892 1965929216 net.cpp:397] relu7 -> fc7 (in-place)
I0102 20:28:42.316383 1965929216 net.cpp:150] Setting up relu7
I0102 20:28:42.316396 1965929216 net.cpp:157] Top shape: 50 4096 (204800)
I0102 20:28:42.316403 1965929216 net.cpp:165] Memory required for data: 414943600
I0102 20:28:42.316408 1965929216 layer_factory.hpp:77] Creating layer drop7
I0102 20:28:42.316416 1965929216 net.cpp:106] Creating Layer drop7
I0102 20:28:42.316437 1965929216 net.cpp:454] drop7 <- fc7
I0102 20:28:42.316449 1965929216 net.cpp:397] drop7 -> fc7 (in-place)
I0102 20:28:42.316511 1965929216 net.cpp:150] Setting up drop7
I0102 20:28:42.316521 1965929216 net.cpp:157] Top shape: 50 4096 (204800)
I0102 20:28:42.316530 1965929216 net.cpp:165] Memory required for data: 415762800
I0102 20:28:42.316546 1965929216 layer_factory.hpp:77] Creating layer fc8_cloudless
I0102 20:28:42.316557 1965929216 net.cpp:106] Creating Layer fc8_cloudless
I0102 20:28:42.316561 1965929216 net.cpp:454] fc8_cloudless <- fc7
I0102 20:28:42.316568 1965929216 net.cpp:411] fc8_cloudless -> fc8_cloudless
I0102 20:28:42.316946 1965929216 net.cpp:150] Setting up fc8_cloudless
I0102 20:28:42.316959 1965929216 net.cpp:157] Top shape: 50 2 (100)
I0102 20:28:42.316965 1965929216 net.cpp:165] Memory required for data: 415763200
I0102 20:28:42.317008 1965929216 layer_factory.hpp:77] Creating layer fc8_cloudless_fc8_cloudless_0_split
I0102 20:28:42.317020 1965929216 net.cpp:106] Creating Layer fc8_cloudless_fc8_cloudless_0_split
I0102 20:28:42.317028 1965929216 net.cpp:454] fc8_cloudless_fc8_cloudless_0_split <- fc8_cloudless
I0102 20:28:42.317037 1965929216 net.cpp:411] fc8_cloudless_fc8_cloudless_0_split -> fc8_cloudless_fc8_cloudless_0_split_0
I0102 20:28:42.317049 1965929216 net.cpp:411] fc8_cloudless_fc8_cloudless_0_split -> fc8_cloudless_fc8_cloudless_0_split_1
I0102 20:28:42.317131 1965929216 net.cpp:150] Setting up fc8_cloudless_fc8_cloudless_0_split
I0102 20:28:42.317153 1965929216 net.cpp:157] Top shape: 50 2 (100)
I0102 20:28:42.317163 1965929216 net.cpp:157] Top shape: 50 2 (100)
I0102 20:28:42.317169 1965929216 net.cpp:165] Memory required for data: 415764000
I0102 20:28:42.317174 1965929216 layer_factory.hpp:77] Creating layer accuracy
I0102 20:28:42.317198 1965929216 net.cpp:106] Creating Layer accuracy
I0102 20:28:42.317205 1965929216 net.cpp:454] accuracy <- fc8_cloudless_fc8_cloudless_0_split_0
I0102 20:28:42.317211 1965929216 net.cpp:454] accuracy <- label_data_1_split_0
I0102 20:28:42.317219 1965929216 net.cpp:411] accuracy -> accuracy
I0102 20:28:42.317242 1965929216 net.cpp:150] Setting up accuracy
I0102 20:28:42.317252 1965929216 net.cpp:157] Top shape: (1)
I0102 20:28:42.317260 1965929216 net.cpp:165] Memory required for data: 415764004
I0102 20:28:42.317267 1965929216 layer_factory.hpp:77] Creating layer loss
I0102 20:28:42.317276 1965929216 net.cpp:106] Creating Layer loss
I0102 20:28:42.317282 1965929216 net.cpp:454] loss <- fc8_cloudless_fc8_cloudless_0_split_1
I0102 20:28:42.317288 1965929216 net.cpp:454] loss <- label_data_1_split_1
I0102 20:28:42.317294 1965929216 net.cpp:411] loss -> loss
I0102 20:28:42.317306 1965929216 layer_factory.hpp:77] Creating layer loss
I0102 20:28:42.317796 1965929216 net.cpp:150] Setting up loss
I0102 20:28:42.317821 1965929216 net.cpp:157] Top shape: (1)
I0102 20:28:42.317831 1965929216 net.cpp:160]     with loss weight 1
I0102 20:28:42.317841 1965929216 net.cpp:165] Memory required for data: 415764008
I0102 20:28:42.317845 1965929216 net.cpp:226] loss needs backward computation.
I0102 20:28:42.317859 1965929216 net.cpp:228] accuracy does not need backward computation.
I0102 20:28:42.317864 1965929216 net.cpp:226] fc8_cloudless_fc8_cloudless_0_split needs backward computation.
I0102 20:28:42.317869 1965929216 net.cpp:226] fc8_cloudless needs backward computation.
I0102 20:28:42.317873 1965929216 net.cpp:226] drop7 needs backward computation.
I0102 20:28:42.317878 1965929216 net.cpp:226] relu7 needs backward computation.
I0102 20:28:42.317881 1965929216 net.cpp:226] fc7 needs backward computation.
I0102 20:28:42.317885 1965929216 net.cpp:228] drop6 does not need backward computation.
I0102 20:28:42.317890 1965929216 net.cpp:228] relu6 does not need backward computation.
I0102 20:28:42.317894 1965929216 net.cpp:228] fc6 does not need backward computation.
I0102 20:28:42.317899 1965929216 net.cpp:228] pool5 does not need backward computation.
I0102 20:28:42.317903 1965929216 net.cpp:228] relu5 does not need backward computation.
I0102 20:28:42.317910 1965929216 net.cpp:228] conv5 does not need backward computation.
I0102 20:28:42.317927 1965929216 net.cpp:228] relu4 does not need backward computation.
I0102 20:28:42.317958 1965929216 net.cpp:228] conv4 does not need backward computation.
I0102 20:28:42.317965 1965929216 net.cpp:228] relu3 does not need backward computation.
I0102 20:28:42.318001 1965929216 net.cpp:228] conv3 does not need backward computation.
I0102 20:28:42.318058 1965929216 net.cpp:228] pool2 does not need backward computation.
I0102 20:28:42.318083 1965929216 net.cpp:228] norm2 does not need backward computation.
I0102 20:28:42.318094 1965929216 net.cpp:228] relu2 does not need backward computation.
I0102 20:28:42.318106 1965929216 net.cpp:228] conv2 does not need backward computation.
I0102 20:28:42.318115 1965929216 net.cpp:228] pool1 does not need backward computation.
I0102 20:28:42.318142 1965929216 net.cpp:228] norm1 does not need backward computation.
I0102 20:28:42.318161 1965929216 net.cpp:228] relu1 does not need backward computation.
I0102 20:28:42.318169 1965929216 net.cpp:228] conv1 does not need backward computation.
I0102 20:28:42.318177 1965929216 net.cpp:228] label_data_1_split does not need backward computation.
I0102 20:28:42.318193 1965929216 net.cpp:228] data does not need backward computation.
I0102 20:28:42.318202 1965929216 net.cpp:270] This network produces output accuracy
I0102 20:28:42.318210 1965929216 net.cpp:270] This network produces output loss
I0102 20:28:42.318243 1965929216 net.cpp:283] Network initialization done.
I0102 20:28:42.318372 1965929216 solver.cpp:60] Solver scaffolding done.
I0102 20:28:42.319234 1965929216 caffe.cpp:128] Finetuning from /Users/bradneuberg/dev/cloudless/src/caffe_model/bvlc_alexnet/bvlc_alexnet.caffemodel
I0102 20:28:42.827081 1965929216 upgrade_proto.cpp:42] Attempting to upgrade input file specified using deprecated transformation parameters: /Users/bradneuberg/dev/cloudless/src/caffe_model/bvlc_alexnet/bvlc_alexnet.caffemodel
I0102 20:28:42.827108 1965929216 upgrade_proto.cpp:45] Successfully upgraded file specified using deprecated data transformation parameters.
W0102 20:28:42.827117 1965929216 upgrade_proto.cpp:47] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0102 20:28:42.827455 1965929216 upgrade_proto.cpp:51] Attempting to upgrade input file specified using deprecated V1LayerParameter: /Users/bradneuberg/dev/cloudless/src/caffe_model/bvlc_alexnet/bvlc_alexnet.caffemodel
I0102 20:28:43.146363 1965929216 upgrade_proto.cpp:59] Successfully upgraded file specified using deprecated V1LayerParameter
I0102 20:28:43.219849 1965929216 net.cpp:816] Ignoring source layer fc8
I0102 20:28:43.657049 1965929216 upgrade_proto.cpp:42] Attempting to upgrade input file specified using deprecated transformation parameters: /Users/bradneuberg/dev/cloudless/src/caffe_model/bvlc_alexnet/bvlc_alexnet.caffemodel
I0102 20:28:43.657078 1965929216 upgrade_proto.cpp:45] Successfully upgraded file specified using deprecated data transformation parameters.
W0102 20:28:43.657083 1965929216 upgrade_proto.cpp:47] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0102 20:28:43.657102 1965929216 upgrade_proto.cpp:51] Attempting to upgrade input file specified using deprecated V1LayerParameter: /Users/bradneuberg/dev/cloudless/src/caffe_model/bvlc_alexnet/bvlc_alexnet.caffemodel
I0102 20:28:43.908476 1965929216 upgrade_proto.cpp:59] Successfully upgraded file specified using deprecated V1LayerParameter
I0102 20:28:43.969301 1965929216 net.cpp:816] Ignoring source layer fc8
I0102 20:28:44.011607 1965929216 caffe.cpp:212] Starting Optimization
I0102 20:28:44.011649 1965929216 solver.cpp:288] Solving AlexNet
I0102 20:28:44.011659 1965929216 solver.cpp:289] Learning Rate Policy: step
I0102 20:28:44.012913 1965929216 solver.cpp:341] Iteration 0, Testing net (#0)
I0102 20:36:28.573385 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.59512
I0102 20:36:28.573828 1965929216 solver.cpp:409]     Test net output #1: loss = 0.630386 (* 1 = 0.630386 loss)
I0102 20:36:29.182698 1965929216 solver.cpp:237] Iteration 0, loss = 0.705415
I0102 20:36:29.182732 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.5625
I0102 20:36:29.182741 1965929216 solver.cpp:253]     Train net output #1: loss = 0.705415 (* 1 = 0.705415 loss)
I0102 20:36:29.183102 1965929216 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0102 20:36:41.929564 1965929216 solver.cpp:237] Iteration 20, loss = 0.219322
I0102 20:36:41.929592 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.9375
I0102 20:36:41.929601 1965929216 solver.cpp:253]     Train net output #1: loss = 0.219322 (* 1 = 0.219322 loss)
I0102 20:36:41.929607 1965929216 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0102 20:36:54.724774 1965929216 solver.cpp:237] Iteration 40, loss = 0.060619
I0102 20:36:54.724804 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.96875
I0102 20:36:54.724814 1965929216 solver.cpp:253]     Train net output #1: loss = 0.060619 (* 1 = 0.060619 loss)
I0102 20:36:54.724822 1965929216 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0102 20:37:00.452929 1965929216 solver.cpp:341] Iteration 50, Testing net (#0)
I0102 20:44:52.175868 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.985099
I0102 20:44:52.176786 1965929216 solver.cpp:409]     Test net output #1: loss = 0.066283 (* 1 = 0.066283 loss)
I0102 20:44:59.347565 1965929216 solver.cpp:237] Iteration 60, loss = 0.0586047
I0102 20:44:59.347596 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.984375
I0102 20:44:59.347610 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0586047 (* 1 = 0.0586047 loss)
I0102 20:44:59.347618 1965929216 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0102 20:45:12.279443 1965929216 solver.cpp:237] Iteration 80, loss = 0.198733
I0102 20:45:12.279485 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.96875
I0102 20:45:12.279500 1965929216 solver.cpp:253]     Train net output #1: loss = 0.198733 (* 1 = 0.198733 loss)
I0102 20:45:12.279520 1965929216 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0102 20:45:24.433727 1965929216 solver.cpp:459] Snapshotting to binary proto file snapshots/bvlc_alexnet_iter_100.caffemodel
I0102 20:45:26.500921 1965929216 sgd_solver.cpp:269] Snapshotting solver state to binary proto file snapshots/bvlc_alexnet_iter_100.solverstate
I0102 20:45:27.516463 1965929216 solver.cpp:341] Iteration 100, Testing net (#0)
I0102 20:53:20.792249 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.986859
I0102 20:53:20.792291 1965929216 solver.cpp:409]     Test net output #1: loss = 0.054345 (* 1 = 0.054345 loss)
I0102 20:53:21.357611 1965929216 solver.cpp:237] Iteration 100, loss = 0.0522739
I0102 20:53:21.357640 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.984375
I0102 20:53:21.357650 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0522739 (* 1 = 0.0522739 loss)
I0102 20:53:21.357657 1965929216 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0102 20:53:34.564862 1965929216 solver.cpp:237] Iteration 120, loss = 0.112744
I0102 20:53:34.564891 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.96875
I0102 20:53:34.564900 1965929216 solver.cpp:253]     Train net output #1: loss = 0.112744 (* 1 = 0.112744 loss)
I0102 20:53:34.564908 1965929216 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0102 20:53:47.750258 1965929216 solver.cpp:237] Iteration 140, loss = 0.00950337
I0102 20:53:47.750293 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 20:53:47.750303 1965929216 solver.cpp:253]     Train net output #1: loss = 0.00950337 (* 1 = 0.00950337 loss)
I0102 20:53:47.750413 1965929216 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0102 20:53:53.715102 1965929216 solver.cpp:341] Iteration 150, Testing net (#0)
I0102 21:01:33.698807 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.989239
I0102 21:01:33.698848 1965929216 solver.cpp:409]     Test net output #1: loss = 0.0457272 (* 1 = 0.0457272 loss)
I0102 21:01:40.566840 1965929216 solver.cpp:237] Iteration 160, loss = 0.0190207
I0102 21:01:40.566867 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:01:40.566876 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0190207 (* 1 = 0.0190207 loss)
I0102 21:01:40.566882 1965929216 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0102 21:01:53.143337 1965929216 solver.cpp:237] Iteration 180, loss = 0.0119353
I0102 21:01:53.143360 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:01:53.143368 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0119353 (* 1 = 0.0119353 loss)
I0102 21:01:53.143374 1965929216 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0102 21:02:05.141042 1965929216 solver.cpp:459] Snapshotting to binary proto file snapshots/bvlc_alexnet_iter_200.caffemodel
I0102 21:02:06.853142 1965929216 sgd_solver.cpp:269] Snapshotting solver state to binary proto file snapshots/bvlc_alexnet_iter_200.solverstate
I0102 21:02:07.871059 1965929216 solver.cpp:341] Iteration 200, Testing net (#0)
I0102 21:09:46.354689 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.991759
I0102 21:09:46.354725 1965929216 solver.cpp:409]     Test net output #1: loss = 0.0429913 (* 1 = 0.0429913 loss)
I0102 21:09:46.914225 1965929216 solver.cpp:237] Iteration 200, loss = 0.00455811
I0102 21:09:46.914252 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:09:46.914259 1965929216 solver.cpp:253]     Train net output #1: loss = 0.00455811 (* 1 = 0.00455811 loss)
I0102 21:09:46.914265 1965929216 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0102 21:09:59.563547 1965929216 solver.cpp:237] Iteration 220, loss = 0.0056713
I0102 21:09:59.563576 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:09:59.563586 1965929216 solver.cpp:253]     Train net output #1: loss = 0.00567131 (* 1 = 0.00567131 loss)
I0102 21:09:59.563593 1965929216 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0102 21:10:12.224390 1965929216 solver.cpp:237] Iteration 240, loss = 0.00475169
I0102 21:10:12.224413 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:10:12.224421 1965929216 solver.cpp:253]     Train net output #1: loss = 0.00475169 (* 1 = 0.00475169 loss)
I0102 21:10:12.224426 1965929216 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0102 21:10:17.926209 1965929216 solver.cpp:341] Iteration 250, Testing net (#0)
I0102 21:17:56.948657 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.990898
I0102 21:17:56.948807 1965929216 solver.cpp:409]     Test net output #1: loss = 0.042281 (* 1 = 0.042281 loss)
I0102 21:18:03.837820 1965929216 solver.cpp:237] Iteration 260, loss = 0.0512488
I0102 21:18:03.837843 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.984375
I0102 21:18:03.837852 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0512489 (* 1 = 0.0512489 loss)
I0102 21:18:03.837858 1965929216 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0102 21:18:16.424082 1965929216 solver.cpp:237] Iteration 280, loss = 0.0144424
I0102 21:18:16.424111 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:18:16.424175 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0144424 (* 1 = 0.0144424 loss)
I0102 21:18:16.424201 1965929216 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0102 21:18:28.383481 1965929216 solver.cpp:459] Snapshotting to binary proto file snapshots/bvlc_alexnet_iter_300.caffemodel
I0102 21:18:30.097082 1965929216 sgd_solver.cpp:269] Snapshotting solver state to binary proto file snapshots/bvlc_alexnet_iter_300.solverstate
I0102 21:18:31.142148 1965929216 solver.cpp:341] Iteration 300, Testing net (#0)
I0102 21:26:11.917461 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.990119
I0102 21:26:11.917505 1965929216 solver.cpp:409]     Test net output #1: loss = 0.0430008 (* 1 = 0.0430008 loss)
I0102 21:26:12.484496 1965929216 solver.cpp:237] Iteration 300, loss = 0.00823956
I0102 21:26:12.484527 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:26:12.484536 1965929216 solver.cpp:253]     Train net output #1: loss = 0.00823957 (* 1 = 0.00823957 loss)
I0102 21:26:12.484544 1965929216 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0102 21:26:25.598621 1965929216 solver.cpp:237] Iteration 320, loss = 0.0300162
I0102 21:26:25.598650 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.984375
I0102 21:26:25.598659 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0300162 (* 1 = 0.0300162 loss)
I0102 21:26:25.598666 1965929216 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0102 21:26:38.519203 1965929216 solver.cpp:237] Iteration 340, loss = 0.0144915
I0102 21:26:38.519233 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:26:38.519243 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0144916 (* 1 = 0.0144916 loss)
I0102 21:26:38.519331 1965929216 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0102 21:26:44.271566 1965929216 solver.cpp:341] Iteration 350, Testing net (#0)
I0102 21:34:31.844521 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.992558
I0102 21:34:31.844569 1965929216 solver.cpp:409]     Test net output #1: loss = 0.0398849 (* 1 = 0.0398849 loss)
I0102 21:34:38.748352 1965929216 solver.cpp:237] Iteration 360, loss = 0.0328946
I0102 21:34:38.748381 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.96875
I0102 21:34:38.748391 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0328946 (* 1 = 0.0328946 loss)
I0102 21:34:38.748399 1965929216 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0102 21:34:51.617786 1965929216 solver.cpp:237] Iteration 380, loss = 0.0104179
I0102 21:34:51.617812 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:34:51.617821 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0104179 (* 1 = 0.0104179 loss)
I0102 21:34:51.617827 1965929216 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0102 21:35:03.832548 1965929216 solver.cpp:459] Snapshotting to binary proto file snapshots/bvlc_alexnet_iter_400.caffemodel
I0102 21:35:05.622586 1965929216 sgd_solver.cpp:269] Snapshotting solver state to binary proto file snapshots/bvlc_alexnet_iter_400.solverstate
I0102 21:35:06.717519 1965929216 solver.cpp:341] Iteration 400, Testing net (#0)
I0102 21:42:53.223410 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.990119
I0102 21:42:53.223821 1965929216 solver.cpp:409]     Test net output #1: loss = 0.0425994 (* 1 = 0.0425994 loss)
I0102 21:42:53.770402 1965929216 solver.cpp:237] Iteration 400, loss = 0.0584194
I0102 21:42:53.770428 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.984375
I0102 21:42:53.770437 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0584194 (* 1 = 0.0584194 loss)
I0102 21:42:53.770442 1965929216 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0102 21:43:06.490387 1965929216 solver.cpp:237] Iteration 420, loss = 0.0304142
I0102 21:43:06.490414 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.984375
I0102 21:43:06.490422 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0304142 (* 1 = 0.0304142 loss)
I0102 21:43:06.490428 1965929216 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0102 21:43:19.390954 1965929216 solver.cpp:237] Iteration 440, loss = 0.00211791
I0102 21:43:19.390982 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:43:19.390991 1965929216 solver.cpp:253]     Train net output #1: loss = 0.00211793 (* 1 = 0.00211793 loss)
I0102 21:43:19.390998 1965929216 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0102 21:43:25.400741 1965929216 solver.cpp:341] Iteration 450, Testing net (#0)
I0102 21:51:11.878756 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.990899
I0102 21:51:11.879667 1965929216 solver.cpp:409]     Test net output #1: loss = 0.0389473 (* 1 = 0.0389473 loss)
I0102 21:51:18.836237 1965929216 solver.cpp:237] Iteration 460, loss = 0.0136283
I0102 21:51:18.836266 1965929216 solver.cpp:253]     Train net output #0: accuracy = 0.984375
I0102 21:51:18.836274 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0136283 (* 1 = 0.0136283 loss)
I0102 21:51:18.836282 1965929216 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0102 21:51:31.509064 1965929216 solver.cpp:237] Iteration 480, loss = 0.0105011
I0102 21:51:31.509090 1965929216 solver.cpp:253]     Train net output #0: accuracy = 1
I0102 21:51:31.509099 1965929216 solver.cpp:253]     Train net output #1: loss = 0.0105011 (* 1 = 0.0105011 loss)
I0102 21:51:31.509104 1965929216 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0102 21:51:43.549726 1965929216 solver.cpp:459] Snapshotting to binary proto file snapshots/bvlc_alexnet_iter_500.caffemodel
I0102 21:51:45.273923 1965929216 sgd_solver.cpp:269] Snapshotting solver state to binary proto file snapshots/bvlc_alexnet_iter_500.solverstate
I0102 21:51:46.850963 1965929216 solver.cpp:321] Iteration 500, loss = 0.00980271
I0102 21:51:46.850999 1965929216 solver.cpp:341] Iteration 500, Testing net (#0)
I0102 21:59:31.610237 1965929216 solver.cpp:409]     Test net output #0: accuracy = 0.991738
I0102 21:59:31.610527 1965929216 solver.cpp:409]     Test net output #1: loss = 0.036659 (* 1 = 0.036659 loss)
I0102 21:59:31.610548 1965929216 solver.cpp:326] Optimization Done.
I0102 21:59:31.610553 1965929216 caffe.cpp:215] Optimization Done.
